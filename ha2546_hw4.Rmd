---
title: "tree-based methods"
author: "Hana Akbarnejad"
date: "4/24/2020"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(viridis)
library(ggplot2)
library(readr)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
library(plotmo)
library(pdp)
library(lime)
library(lasso2) #for data
library(ISLR)

knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

```{r}
data(Prostate)
prostate_data = Prostate
ctrl = trainControl(method = "cv")
```

(a) Fit a regression tree with lpsa as the response and the other variables as predictors.
Use cross-validation to determine the optimal tree size. Which tree size corresponds
to the lowest cross-validation error? Is this the same as the tree size obtained using
the 1 SE rule?
```{r}
set.seed(2020)

# fitting initial tree
tree1 = rpart(formula = lpsa~., data = prostate_data)
rpart.plot(tree1)

# using CV to find optimal tree size
cp_table = printcp(tree1) # smallest error of 0.60554 corresponds to cp of  0.021470 which is tree size 7
plotcp(tree1)

#prune tree with obtained cp:
min_error = which.min(cp_table[,4])   # shows that minimum error belongs to tree size 7
# minimum cross-validation error
tree2 = prune(tree1, cp = cp_table[min_error,1])
rpart.plot(tree2)

# 1SE rule...
tree3 = prune(tree1, cp = cp_table[cp_table[,4]<cp_table[min_error,4]+cp_table[min_error,5],1][1])
rpart.plot(tree3) # 1SE rule shows tree size of 4 which is different from cp method
```

It can be observed that he tree obtained with the minimum CV error has different size compared to the tree that is obtained from the 1 SE rule. According to the minimum CV error, the optimal tree size is 7, while the size fo the tree obtained from 1 SE rule is 4.

(b) Create a plot of the final tree you choose.

Looking at the Cp plot created above, it can be observed that the file point below horizental line belongs to the cp of 0.045 and tree size 4 which is the same as what we got from 1 SE rule. So, I prune the tree using this cp value and choose it as my final tree. Chosing higher complexity results in a smaller and more interpretible tree.
```{r}
set.seed(2020)
tree4 =  rpart(formula = lpsa~., data = prostate_data,
               control = rpart.control(cp = 0.045))
rpart.plot(tree4)
```

Pick one of the terminal nodes, and interpret the information displayed......................

(c) Perform bagging and report the variable importance.
```{r}
set.seed(2020)
bagging = randomForest(lpsa ~ ., data = prostate_data, mtry = 8)
bagging2 = ranger(lpsa ~ ., data = prostate_data, mtry = 8,
             splitrule = "variance",
             importance = "permutation",
             scale.permutation.importance = TRUE)
importance(bagging2)

barplot(sort(ranger::importance(bagging2), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))
```

Explain finding........

(d) Perform random forests and report the variable importance.
```{r}
set.seed(2020)
rf = randomForest(lpsa ~ ., data = prostate_data, mtry = 2)
rf2 = ranger(lpsa ~ ., data = prostate_data, mtry = 2,
             splitrule = "variance",
             importance = "permutation",
             scale.permutation.importance = TRUE)
importance(rf2)
barplot(sort(ranger::importance(rf2), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))
```

Explain Findings............

(e) Perform boosting and report the variable importance.
```{r}

set.seed(2020)
boosting = gbm(lpsa ~ ., prostate_data,
          distribution = "gaussian",
          n.trees = 5000,
          interaction.depth = 3,
          shrinkage = 0.005,
          cv.folds = 10)

nt = gbm.perf(boosting, method = "cv") # optimal number of trees

# Grid search and find optimal tuning parameter
boosting_grid = expand.grid(n.trees = c(2000,3000),
                      interaction.depth = 2:10,
                      shrinkage = c(0.001,0.003,0.005),
                      n.minobsinnode = 1)

set.seed(2020)
boosting_fit = train(lpsa ~ ., prostate_data,
                method = "gbm",
                tuneGrid = boosting_grid,
                trControl = ctrl,
                verbose = FALSE)

# summary of gbm from caret gives the variable importance for boosting
summary(boosting_fit$finalModel, las = 2, cBars = 8, cex.names = 0.6)
```

(f) Which of the above models will you select to predict PSA level? Explain.
```{r}

# RF using caret
rf_grid = expand.grid(mtry = 1:2,
                      splitrule = "variance",
                      min.node.size = 1:2)
set.seed(2020)
rf_fit = train(lpsa ~ ., prostate_data,
                method = "ranger",
                tuneGrid = rf_grid,
                trControl = ctrl)

# bagging using caret
bagging_grid = expand.grid(mtry = 1:8,
                      splitrule = "variance",
                      min.node.size = 1:8)
set.seed(2020)
bagging_fit = train(lpsa ~ ., prostate_data,
                method = "ranger",
                tuneGrid = bagging_grid,
                trControl = ctrl)

resamp = resamples(list(RF = rf_fit, boosting = boosting_fit, bagging = bagging_fit))
summary(resamp) # bagging with Mean RMSE of 0.7227003 is has the lowest error
```

Bagging?

## Problem 2
```{r}
data(OJ)
oj_data = OJ
oj_data = oj_data %>% 
  janitor::clean_names() %>% 
  mutate(
    purchase = as.factor(purchase)
  )

train_data = oj_data[1:800,]
test_data = oj_data[801:1070,]

ctrl2 = trainControl(method = "repeatedcv",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

(a) Fit a classification tree to the training set, with Purchase as the response and the
other variables as predictors. Use cross-validation to determine the tree size and
create a plot of the final tree. Predict the response on the test data. What is the test
classification error rate?

```{r}
set.seed(2020)

class_tree1 = rpart(formula = purchase~.,
                    data = train_data,
                    control = rpart.control(cp = 0))

rpart.plot(class_tree1)

# min CV error
class_cp_table = printcp(class_tree1)
plotcp(class_tree1)
class_min_err = which.min(class_cp_table[,4])

# pruning
class_tree2 = prune(class_tree1, cp = class_cp_table[class_min_err,1])
rpart.plot(class_tree2) #10 end nodes


# ctrl = trainControl(method = "repeatedcv",
# summaryFunction = twoClassSummary,
# classProbs = TRUE)
# set.seed(1)
# rpart.fit <- train(diabetes~., dat,
# subset = rowTrain,
# method = "rpart",
# tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 20))),
# trControl = ctrl,
# metric = "ROC")
# ggplot(rpart.fit, highlight = TRUE)
# rpart.plot(rpart.fit$finalModel)

# prediction on test data
rpart_pred = predict(class_tree2, newdata = test_data)[,1]
ModelMetrics::rmse(rpart_pred, test_data$purchase) # test error is  1.133502 (???)
```

Explain findings...........

(b) Perform random forests on the training set and report variable importance. What is
the test error rate?

```{r}
class_rf = ranger(purchase~., test_data,
                  mtry = 5,
                  min.node.size = 5,
                  splitrule = "gini",
                  importance = "permutation",
                  scale.permutation.importance = TRUE)

barplot(sort(ranger::importance(class_rf), decreasing = FALSE),
                  las = 2, horiz = TRUE, cex.names = 0.7,
                  col = colorRampPalette(colors = c("darkred", "white", "darkblue"))(17))

class_rf_pred = predict(class_rf, data = test_data, type = "response")$predictions
ModelMetrics::rmse(class_rf_pred, test_data$purchase) # mse is 0.2357023
```

Explain findings......

(c) Perform boosting on the training set and report variable importance. What is the
test error rate?
```{r}

class_gbm_grid = expand.grid(n.trees = 3000,
                        interaction.depth = 1:6,
                        shrinkage = 0.001,
                        n.minobsinnode = 1)
set.seed(2020)
# Binomial loss function
class_gbm_fit = train(purchase~.,
                  data = train_data,
                  tuneGrid = class_gbm_grid,
                  trControl = ctrl2,
                  method = "gbm",
                  distribution = "bernoulli",
                  metric = "ROC",
                  verbose = FALSE)

summary(class_gbm_fit$finalModel, las = 2, cBars = 8, cex.names = 0.6)

class_gbm_pred = predict(class_gbm_fit, newdata = test_data, type = "prob")[,1]
ModelMetrics::rmse(class_gbm_pred, test_data$purchase) #1.099664
```

Explain findings .................