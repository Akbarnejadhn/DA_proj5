---
title: "Tree-based Methods"
author: "Hana Akbarnejad"
date: "4/26/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(ggplot2)
library(readr)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
library(plotmo)
library(pdp)
library(lime)
library(lasso2) #for data
library(ISLR)

knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

```{r include=FALSE}
data(Prostate)
prostate_data = Prostate
ctrl = trainControl(method = "cv")
```

## Problem 1

### a
Fit a regression tree with lpsa as the response and the other variables as predictors. Use cross-validation to determine the optimal tree size. Which tree size corresponds to the lowest cross-validation error? Is this the same as the tree size obtained using the 1 SE rule?
```{r}

set.seed(2020)

# fitting initial tree
tree1 = rpart(formula = lpsa~., data = prostate_data)
rpart.plot(tree1)

# using CV to find optimal tree size
cp_table = printcp(tree1) # smallest error of 0.60554 corresponds to cp of  0.021470 which is tree size 7
plotcp(tree1)

#prune tree with obtained cp:
min_error = which.min(cp_table[,4])   # shows that minimum error belongs to tree size 7
# minimum cross-validation error
tree2 = prune(tree1, cp = cp_table[min_error,1])
rpart.plot(tree2)

# 1SE rule...
tree3 = prune(tree1, cp = cp_table[cp_table[,4]<cp_table[min_error,4]+cp_table[min_error,5],1][1])
rpart.plot(tree3) # 1SE rule shows tree size of 4 which is different from cp method
```

It can be observed that he tree obtained with the minimum CV error has different size compared to the tree that is obtained from the 1 SE rule. According to the minimum CV error, the optimal tree size is 7, while the size fo the tree obtained from 1 SE rule is 4.

### b
Create a plot of the final tree you choose. Pick one of the terminal nodes, and interpret the information displayed.

Looking at the Cp plot created above, it can be observed that the file point below horizental line belongs to the cp of 0.045 and tree size 4 which is the same as what we got from 1 SE rule. So, I prune the tree using this cp value and choose it as my final tree. Chosing higher complexity results in a smaller and more interpretible tree.

```{r}
set.seed(2020)
tree4 =  rpart(formula = lpsa~., data = prostate_data,   # refit it in caret
               control = rpart.control(cp = 0.045))
rpart.plot(tree4)
```

We can observe that there are four terminal nodes which have 9%, 39%, 30%, and 22% of observations, from left to right. The other number in terminal nodes contain the average response level of the observations that fall within tat node. For example, the righter most node contains 22% of observations with mean IPSA level of 3.8. The splitting steps of trees were based on lcavol and lweight values cut-offs.

### c
Perform bagging and report the variable importance.
```{r}
set.seed(2020)
bagging = randomForest(lpsa ~ ., data = prostate_data, mtry = 8)
bagging2 = ranger(lpsa ~ ., data = prostate_data, mtry = 8,
             splitrule = "variance",
             importance = "permutation",
             scale.permutation.importance = TRUE)

importance(bagging2)

barplot(sort(ranger::importance(bagging2), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))
```

After performing Bagging method, the variable importance value can be observed above for each variable. Also, the above barplot shows that top three variables are *lcavol*, *lweight*, and *svi*.

### d
Perform random forests and report the variable importance.

```{r}
set.seed(2020)
rf = randomForest(lpsa ~ ., data = prostate_data, mtry = 2)
rf2 = ranger(lpsa ~ ., data = prostate_data, mtry = 2, #mtry
             splitrule = "variance",
             importance = "permutation",
             scale.permutation.importance = TRUE)

importance(rf2)
barplot(sort(ranger::importance(rf2), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(8))
```

After performing Random Forests method, the variable importance value can be observed above for each variable. Also, the above barplot shows that top three variables are *lcavol*, *svi*, and *lweight*; the same variables as obtained through Bagging.

### e
Perform boosting and report the variable importance.
```{r}
set.seed(2020)
boosting = gbm(lpsa ~ ., prostate_data,
          distribution = "gaussian",
          n.trees = 5000,
          interaction.depth = 3,
          shrinkage = 0.005,
          cv.folds = 10)

nt = gbm.perf(boosting, method = "cv") # optimal number of trees

# Grid search and find optimal tuning parameter
boosting_grid = expand.grid(n.trees = c(2000,3000),
                            interaction.depth = 2:10,
                            shrinkage = c(0.001,0.003,0.005),
                            n.minobsinnode = 1)

set.seed(2020)
boosting_fit = train(lpsa ~ ., prostate_data,
                method = "gbm",
                tuneGrid = boosting_grid,
                trControl = ctrl,
                verbose = FALSE)

# summary of gbm from caret gives the variable importance for boosting
summary(boosting_fit$finalModel, las = 2, cBars = 8, cex.names = 0.6)

boosting_fit$finalModel$tuneValue
```

After performing Boosting method, the variable importance value can be observed above for each variable. Also, the above barplot shows that top three variables are *lcavol*, *lweight*, and *svi*; the same variables as obtained through Bagging and Random Forests.

### f
Which of the above models will you select to predict PSA level? Explain.

```{r}

# RF using caret
rf_grid = expand.grid(mtry = 1:7,
                      splitrule = "variance",
                      min.node.size = 1:15)
set.seed(2020)
rf_fit = train(lpsa ~ ., prostate_data,
                method = "ranger",
                tuneGrid = rf_grid,
                trControl = ctrl)

# bagging using caret
bagging_grid = expand.grid(mtry = 8, 
                      splitrule = "variance",
                      min.node.size = 1:15)
set.seed(2020)
bagging_fit = train(lpsa ~ ., prostate_data,
                method = "ranger",
                tuneGrid = bagging_grid,
                trControl = ctrl)

resamp = resamples(list(RF = rf_fit, boosting = boosting_fit, bagging = bagging_fit))
summary(resamp) # boosting with Mean RMSE of 0.7341767 is has the lowest error
```

After comparing the three ensamble methods in caret and comapring them using resmples method, we can see that these methods have very close Mean RMSE, but RF and boosting show the smallest training error (0.74). So, when predicting PSA level, these are the best ensamble method we can choose.